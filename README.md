# Ensemble-Classifier-for-Churn-Prediction-in-R

I do not own the credits for inventing this technique. I have made certain modifications on the original paper 'Key-Attributes-Based Ensemble Classifier for Customer Churn Prediction' by Yu Qian, Liang-Qiang Li, Jian-Rong Ran, and Pei-Ji Shao.

In this project I have modified the methods described. The key changes I made to the paper were: the preprocessing involved, the feature selection algorithm, approach to class imbalance, generalized sub-dataset formation, adding tweaking factors to, introducing a neutral classifier in each pipeline and re-engineered the ensemble algorithm.

The data (Asian telecom markets) had over a million rows and 200++ attributes. Not all million customers were valuable, filtering the data by 70th percentile of revenue earned earned I got around 30,000 rows. Some feature engineering reduced the attributes to 205. I wanted to try Lasso or hierarchical clustering to agglomerate the 200+ features into 4 but settled with the sklearn function 'featureagglomeration' which lets me compare L1, L2 with the standard Euclidian regularization. The algorithm gives 4 new features that represent the entire data, inversing this operation gave the 4 clusters. To each of the 4 clusters I applied SDV to distribute the rows over 3 axes to create 3 sub datasets with different ranges of the same data, each based on the key feature that influences the cluster. This gives us 12 sub datasets which were split into train and test in a 0.7 ratio.

Now for the classifiers, I created wrapper functions for Na√Øve Bayes(base) which gives 0.5 accuracy throughout, C4.5 decision tree(party) and weighted SVM(e1071) that returned S4 objects of the trained models and had hyperparameters for fine tuning to each sub-dataset. These were wrapped into another wrapper for the training pipeline. Finally the test data is passed through a pipe created with all 12 classifiers and their respective weights giving 12 predictions which are polled. This pipe is fed the data by a wrapper which decides which sub-dataset best suits the test data. and finally one that iterates this over the whole test set. The output is given in terms of 0 for no-churn and 1 for churn.
